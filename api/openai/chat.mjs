// OpenAI Chat Completions Mock API - Complete Implementation
// Vercel Serverless í™˜ê²½ì— ìµœì í™”ëœ ì „ì²´ ê¸°ëŠ¥ êµ¬í˜„

// Content Samples (openai-api-mockê³¼ ë™ì¼)
const CONTENT_SAMPLES = {
  markdown: [
    "# ë§ˆí¬ë‹¤ìš´ ì˜ˆì‹œ\n\n- ì²´í¬ë¦¬ìŠ¤íŠ¸\n- [x] ì™„ë£Œëœ í•­ëª©\n- [ ] ë¯¸ì™„ë£Œ í•­ëª©\n\n```javascript\nconst code = 'example';\nconsole.log('Hello World');\n```\n\n**êµµì€ ê¸€ì”¨**ì™€ *ê¸°ìš¸ì„*ë„ ì§€ì›í•©ë‹ˆë‹¤.",
    "## í…Œì´ë¸” ì˜ˆì‹œ\n\n| ì´ë¦„ | ë‚˜ì´ | ì§ì—… |\n|------|------|------|\n| ê¹€ì² ìˆ˜ | 30 | ê°œë°œì |\n| ì´ì˜í¬ | 28 | ë””ìì´ë„ˆ |\n| ë°•ë¯¼ìˆ˜ | 35 | ê¸°íšì |\n\n> ì¸ìš©ë¬¸ë„ ì§€ì›ë©ë‹ˆë‹¤.",
    "### í•  ì¼ ëª©ë¡\n\n1. [x] í”„ë¡œì íŠ¸ ì„¤ì •\n2. [ ] UI êµ¬í˜„\n3. [ ] í…ŒìŠ¤íŠ¸ ì‘ì„±\n4. [ ] ë°°í¬ ì¤€ë¹„\n\n---\n\n```python\ndef hello():\n    return \"Hello, World!\"\n```"
  ],
  html: [
    "<div><h3>HTML ì˜ˆì‹œ</h3><button style='padding:8px 16px; background:#007bff; color:white; border:none; border-radius:4px; cursor:pointer;'>í´ë¦­ ë²„íŠ¼</button><br><br><ul><li>ëª©ë¡ 1</li><li>ëª©ë¡ 2</li></ul><p><strong>êµµì€ ê¸€ì”¨</strong>ì™€ <em>ê¸°ìš¸ì„</em></p></div>",
    "<form style='border:1px solid #ddd; padding:16px; border-radius:8px;'><h4>ì‚¬ìš©ì ì •ë³´ ì…ë ¥</h4><label>ì´ë¦„: <input type='text' placeholder='ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”' style='margin-left:8px; padding:4px;'></label><br><br><label>ì´ë©”ì¼: <input type='email' placeholder='email@example.com' style='margin-left:8px; padding:4px;'></label><br><br><button type='submit' style='background:#28a745; color:white; border:none; padding:8px 16px; border-radius:4px;'>ì „ì†¡</button></form>",
    "<div class='alim_box' style='background:#f8f9fa; padding:16px; border-left:4px solid #007bff;'><h4 style='margin:0 0 8px 0; color:#007bff;'>ì•Œë¦¼</h4><p style='margin:0;'>ì´ê²ƒì€ HTML ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ ì•Œë¦¼ ë°•ìŠ¤ì…ë‹ˆë‹¤.</p><ul style='margin:8px 0 0 0;'><li>í•­ëª© A</li><li>í•­ëª© B</li></ul></div>"
  ],
  json: [
    '{\n  "user": {\n    "name": "ê¹€ì² ìˆ˜",\n    "age": 30,\n    "skills": ["React", "TypeScript", "Node.js"],\n    "isActive": true\n  },\n  "timestamp": "2024-01-15T09:30:00Z",\n  "status": "success"\n}',
    '{\n  "products": [\n    {\n      "id": 1,\n      "name": "ë…¸íŠ¸ë¶",\n      "price": 1200000,\n      "category": "ì „ìì œí’ˆ"\n    },\n    {\n      "id": 2,\n      "name": "ë§ˆìš°ìŠ¤",\n      "price": 50000,\n      "category": "ì£¼ë³€ê¸°ê¸°"\n    }\n  ],\n  "total": 1250000,\n  "currency": "KRW"\n}',
    '{\n  "response": {\n    "data": {\n      "users": [\n        {"id": 1, "name": "Alice", "role": "admin"},\n        {"id": 2, "name": "Bob", "role": "user"}\n      ]\n    },\n    "meta": {\n      "page": 1,\n      "limit": 10,\n      "total": 2\n    }\n  }\n}'
  ],
  text: [
    "ì•ˆë…•í•˜ì„¸ìš”! ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ì´ë‚˜ HTML íƒœê·¸ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.",
    "ì´ê²ƒì€ í‰ë²”í•œ ëŒ€í™”í˜• ì‘ë‹µì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ í˜•íƒœì˜ ë©”ì‹œì§€ì…ë‹ˆë‹¤.",
    "AI ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì œê³µí•˜ëŠ” ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤. íŠ¹ë³„í•œ í¬ë§·íŒ… ì—†ì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤."
  ]
};

// Stream states storage (ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì—ì„œëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥)
const streamStates = new Map();

// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
function getRandomElement(array) {
  return array[Math.floor(Math.random() * array.length)];
}

function generateId() {
  return Math.random().toString(36).substr(2, 30);
}

// Content type ê°ì§€ (openai-api-mockê³¼ ë™ì¼)
function detectContentType(messages) {
  const lastMessage = messages[messages.length - 1]?.content?.toLowerCase() || '';
  
  if (lastMessage.includes('markdown') || lastMessage.includes('md')) {
    return 'markdown';
  }
  if (lastMessage.includes('html')) {
    return 'html';
  }
  if (lastMessage.includes('json')) {
    return 'json';
  }
  return 'text';
}

// Content sample ë°˜í™˜
function getContentSample(contentType) {
  const samples = CONTENT_SAMPLES[contentType] || CONTENT_SAMPLES.text;
  return getRandomElement(samples);
}

// í† í° ê³„ì‚° (ëŒ€ëµì ì¸ ê³„ì‚°)
function calculateTokens(text) {
  return Math.floor(text.length / 4);
}

// Stream cleanup
function cleanupStream(streamId) {
  if (streamId && streamStates.has(streamId)) {
    streamStates.delete(streamId);
  }
}

// OpenAI ì‘ë‹µ ìƒì„± (ì „ì²´ ê¸°ëŠ¥ í¬í•¨)
function createChatResponse(content, contentType, streaming = false, streamId = null, isFirst = false, isLast = false) {
  const id = streamId || `chatcmpl-${generateId()}`;
  const created = Math.floor(Date.now() / 1000);
  
  const baseResponse = {
    id,
    object: streaming ? 'chat.completion.chunk' : 'chat.completion',
    created,
    model: 'gpt-3.5-turbo-0125',
    system_fingerprint: null,
    choices: [{
      index: 0,
      logprobs: null,
      finish_reason: null
    }]
  };

  if (streaming) {
    // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    baseResponse.choices[0].delta = {
      content: content || ''
    };
    
    // ì²« ë²ˆì§¸ ì²­í¬ì—ëŠ” role ì¶”ê°€
    if (isFirst) {
      baseResponse.choices[0].delta.role = 'assistant';
      baseResponse.choices[0].delta.contentType = contentType;
    }
    
    // ë§ˆì§€ë§‰ ì²­í¬
    if (isLast) {
      baseResponse.choices[0].finish_reason = 'stop';
      baseResponse.choices[0].delta.contentType = contentType;
    }
  } else {
    // ì¼ë°˜ ì‘ë‹µ
    baseResponse.choices[0].message = {
      role: 'assistant',
      content,
      contentType
    };
    baseResponse.choices[0].finish_reason = 'stop';
    baseResponse.usage = {
      prompt_tokens: 57,
      completion_tokens: calculateTokens(content),
      total_tokens: 57 + calculateTokens(content)
    };
  }

  return baseResponse;
}

// ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í•¨ìˆ˜
async function handleStreaming(req, res, content, contentType, options = {}) {
  const streamId = `chatcmpl-${generateId()}`;
  let isAborted = false;
  
  // Stream state ì´ˆê¸°í™”
  streamStates.set(streamId, {
    content,
    contentType,
    index: 0,
    created: Date.now()
  });

  // Abort ì²˜ë¦¬ (req.on('close') ì‚¬ìš©)
  if (req && req.on) {
    req.on('close', () => {
      isAborted = true;
      cleanupStream(streamId);
    });
  }

  // í—¤ë” ì„¤ì •
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  res.setHeader('X-Accel-Buffering', 'no');

  try {
    // ì²« ë²ˆì§¸ ì²­í¬ (roleê³¼ contentType í¬í•¨)
    const firstChunk = createChatResponse('', contentType, true, streamId, true, false);
    res.write(`data: ${JSON.stringify(firstChunk)}\n\n`);

    // Character-by-character ìŠ¤íŠ¸ë¦¬ë°
    for (let i = 0; i < content.length; i++) {
      if (isAborted) break;

      const char = content[i];
      const chunk = createChatResponse(char, contentType, true, streamId, false, false);
      res.write(`data: ${JSON.stringify(chunk)}\n\n`);

      // ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì§€ì—°
      const delay = options.latency || (char === ' ' ? 10 : 20);
      await new Promise(resolve => setTimeout(resolve, delay));
    }

    if (!isAborted) {
      // ë§ˆì§€ë§‰ ì²­í¬ (finish_reason í¬í•¨)
      const lastChunk = createChatResponse('', contentType, true, streamId, false, true);
      res.write(`data: ${JSON.stringify(lastChunk)}\n\n`);
      res.write('data: [DONE]\n\n');
    }
  } finally {
    cleanupStream(streamId);
    res.end();
  }
}

// ë©”ì¸ í•¸ë“¤ëŸ¬
export default async function handler(req, res) {
  console.log('ğŸš€ Mock Chat API Handler called');
  
  if (req.method !== 'POST') {
    return res.status(405).json({ error: { message: 'Method not allowed' } });
  }

  try {
    const body = req.body;
    const { model, messages, stream, temperature, max_tokens, tools, functions } = body;
    
    // Request validation
    if (!model || !messages || !Array.isArray(messages)) {
      return res.status(400).json({
        error: {
          message: 'Invalid request. Missing required fields.',
          type: 'invalid_request_error',
          code: 'missing_required_fields'
        }
      });
    }

    console.log(`ğŸ“ Processing ${messages.length} messages, streaming: ${stream}`);
    
    // Error simulation (5% í™•ë¥ )
    const includeErrors = process.env.MOCK_INCLUDE_ERRORS === 'true';
    if (includeErrors && Math.random() < 0.05) {
      return res.status(429).json({
        error: {
          message: 'Rate limit exceeded',
          type: 'rate_limit_error',
          code: 'rate_limit_exceeded'
        }
      });
    }

    // Function/Tool calling ì§€ì›
    if (tools || functions) {
      const toolResponse = {
        id: `chatcmpl-${generateId()}`,
        object: 'chat.completion',
        created: Math.floor(Date.now() / 1000),
        model,
        system_fingerprint: null,
        choices: [{
          index: 0,
          message: {
            role: 'assistant',
            content: null,
            tool_calls: tools ? [{
              id: `call_${generateId()}`,
              type: 'function',
              function: {
                name: tools[0].function.name,
                arguments: '{}'
              }
            }] : null,
            function_call: functions ? {
              name: functions[0].name,
              arguments: '{}'
            } : null
          },
          logprobs: null,
          finish_reason: 'tool_calls'
        }],
        usage: {
          prompt_tokens: 57,
          completion_tokens: 17,
          total_tokens: 74
        }
      };
      return res.json(toolResponse);
    }

    // Content type ê°ì§€
    const contentType = detectContentType(messages);
    console.log(`ğŸ­ Detected content type: ${contentType}`);
    
    // Mock content ê°€ì ¸ì˜¤ê¸°
    const mockContent = getContentSample(contentType);
    
    // Latency simulation
    const latency = parseInt(process.env.MOCK_LATENCY || '0');
    if (latency > 0) {
      await new Promise(resolve => setTimeout(resolve, latency));
    }
    
    if (stream) {
      // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
      await handleStreaming(req, res, mockContent, contentType, { latency: 20 });
    } else {
      // ì¼ë°˜ ì‘ë‹µ
      const response = createChatResponse(mockContent, contentType, false);
      res.json(response);
    }
  } catch (err) {
    console.error('âŒ Mock API Error:', err);
    res.status(500).json({
      error: {
        message: 'Internal server error',
        type: 'server_error',
        code: 'internal_error'
      }
    });
  }
}